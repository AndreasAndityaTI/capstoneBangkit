# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Sve8bEM4foq9C7tF0os8IaOzYZyexMI
"""

from google.colab import drive
drive.mount('/content/drive')

zip_path = '/content/drive/My Drive/Citra BISINDO.zip'

import zipfile

extracted_path = '/content/BISINDO'  # Ganti dengan path tempat Anda ingin mengekstrak file ZIP

# Ekstraksi file ZIP
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_path)



import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam



# Sample data
texts = [chr(ord('a') + i) for i in range(26)] + [chr(ord('A') + i) for i in range(26)]
sign_language_images = '/content/BISINDO'

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
total_words = len(tokenizer.word_index) + 1

# Create sequences
input_sequences = tokenizer.texts_to_sequences(texts)
max_sequence_length = max(len(seq) for seq in input_sequences)
padded_input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')

# Convert images to numpy array
image_data = np.array(sign_language_images)

# Split the data into train, validation, and test sets
train_size = int(0.7 * len(texts))
val_size = int(0.15 * len(texts))
test_size = len(texts) - train_size - val_size

train_texts, val_texts, test_texts = texts[:train_size], texts[train_size:train_size+val_size], texts[-test_size:]
train_images, val_images, test_images = image_data[:train_size], image_data[train_size:train_size+val_size], image_data[-test_size:]

# Define the model
model = Sequential()
model.add(Embedding(input_dim=total_words, output_dim=64, input_length=max_sequence_length))
model.add(LSTM(100))
model.add(Dense(64*64*3, activation='linear'))  # Adjust the output dimension
model.add(Reshape((64, 64, 3)))  # Reshape to match the ground truth label shape

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

# Data generators
def data_generator(texts, images, tokenizer, max_sequence_length, batch_size):
    while True:
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_images = images[i:i + batch_size]

            input_sequences = tokenizer.texts_to_sequences(batch_texts)
            padded_input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')

            yield padded_input_sequences, batch_images

batch_size = 1
train_generator = data_generator(train_texts, train_images, tokenizer, max_sequence_length, batch_size)
val_generator = data_generator(val_texts, val_images, tokenizer, max_sequence_length, batch_size)
test_generator = data_generator(test_texts, test_images, tokenizer, max_sequence_length, batch_size)

# Train the model using data generators
epochs = 10
steps_per_epoch = len(train_texts) // batch_size
validation_steps = len(val_texts) // batch_size




model.fit(train_generator,steps_per_epoch=steps_per_epoch ,epochs=epochs, validation_data=val_generator)

# Evaluate the model
test_steps = len(test_texts) // batch_size
evaluation = model.evaluate(test_generator, steps=test_steps)

print(f"Test Accuracy: {evaluation}")

# Evaluate the model
test_steps = len(test_texts) // batch_size
evaluation = model.evaluate(test_generator, steps=test_steps)

print(f"Test Accuracy: {evaluation}")